{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PALE & LINE代码实现的核心算法和思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两篇论文及源码的核心思想解读，通过模拟的方式进行计算实现（in other words, 没有读入图）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 核心算法一：fast_sigmoid和adagrad对节点向量的实现过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fast_sigmoid: 由于原版sigmoid计算耗时，所以采用近似代替或者table的方式加速计算<br>\n",
    "近似代替： 用x/1+|x|代替，时间开销是原来的0.05%~~0.1%，误差很小。<br>\n",
    "table:也就是源代码采用的方式。<br>\n",
    "    建立table: \n",
    "        指定table_size = s, 计算上界bound = b\n",
    "        for k in s: x = 2*b*k/s-b  table[k] = sigmoid(x)<br>\n",
    "    计算某个val时查找table:\n",
    "        val>b, sigmoid(val)=1-epsilon\n",
    "        val<-b,sigmoid(val)=epsilon\n",
    "        k = (val+b)*s/(2*b), sigmoid(val) = table[k]<br>\n",
    "还是不太懂...\n",
    "\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_sigmoid(x):\n",
    "    return x/(1+np.abs(x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.power(np.e,-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed()\n",
    "lr = .001\n",
    "epsilon = 1e-7\n",
    "BATCH_SIZE = 128\n",
    "REP_SIZE = 128\n",
    "NEG_RATIO = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast sigmoid uses 0.004373000000000071 clk and sigmoid uses 8.299999999994423e-05 clk, difference is 1.9834429448417075\n"
     ]
    }
   ],
   "source": [
    " #模拟生成正负样本\n",
    "    pos_u = np.random.rand(BATCH_SIZE,REP_SIZE) \n",
    "    pos_v = np.random.rand(BATCH_SIZE,REP_SIZE)\n",
    "    neg_u = np.random.randn(BATCH_SIZE,NEG_RATIO,REP_SIZE)   #(batch_size,neg_ratio,rep_size)\n",
    "    neg_v = np.random.randn(BATCH_SIZE,NEG_RATIO,REP_SIZE)\n",
    "    posNum = pos_u.shape[0]\n",
    "    negNum = neg_u.shape[0]*neg_u.shape[1]\n",
    "    #calc u*v \n",
    "    pos_sum = np.sum(pos_u*pos_v,axis=1).reshape(-1,1) #(batch_size,1)\n",
    "    neg_sum = np.sum(neg_u*neg_v, axis=2) #(batch_size, neg_ratio)\n",
    "    #calc fast_sigmoid(u*v)\n",
    "    start1 = time.clock()\n",
    "    fast_sigmoid_pos_uv = np.array([fast_sigmoid(x) for x in pos_sum]).reshape(pos_sum.shape) #(batch_size,1)\n",
    "    fast_sigmoid_neg_uv = np.array([fast_sigmoid(x) for x in neg_sum.reshape(-1,1)]).reshape(neg_sum.shape) #(batch_size, neg_ratio)\n",
    "    end1 = time.clock()\n",
    "    t1 = end1-start1\n",
    "    #calc sigmoid(u*v)\n",
    "    start2 = time.clock()\n",
    "    sigmoid_pos_uv = sigmoid(pos_sum).reshape(pos_sum.shape)\n",
    "    sigmoid_neg_uv = sigmoid(neg_sum.reshape(-1,1)).reshape(neg_sum.shape)\n",
    "    end2 = time.clock()\n",
    "    t2 = end2-start2\n",
    "    # difference between fast_sigmoid and sigmoid\n",
    "    diff = np.sum(np.power(fast_sigmoid_neg_uv-sigmoid_neg_uv,2)/posNum+np.power(fast_sigmoid_pos_uv-sigmoid_pos_uv,2)/negNum)\n",
    "    print(\"fast sigmoid uses {} clk and sigmoid uses {} clk, difference is {}\".format(t1, t2, diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.487565969302136\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    #Gradient Descend\n",
    "    grad_pos_u = np.array([(fast_sigmoid_pos_uv[i]-1)*pos_v[i,:] for i in range(posNum)])/posNum     #(batch_size, 128)\n",
    "    grad_pos_v = np.array([(fast_sigmoid_pos_uv[i]-1)*pos_u[i,:] for i in range(posNum)])/negNum      #(batch_size, 128)\n",
    "    grad_neg_u = list()\n",
    "    for i in range(neg_u.shape[0]):\n",
    "        for j in range(neg_u.shape[1]):\n",
    "            grad_neg_u.append(fast_sigmoid_neg_uv[i][j]*neg_v[i,j,:])\n",
    "    grad_neg_u = np.array(grad_neg_u)  #(128*5,128)\n",
    "    grad_neg_v = list()\n",
    "    for i in range(neg_u.shape[0]):\n",
    "        for j in range(neg_u.shape[1]):\n",
    "            grad_neg_v.append(fast_sigmoid_neg_uv[i][j]*neg_u[i,j,:])\n",
    "    grad_neg_v = np.array(grad_neg_v) #(128*5,128)\n",
    "    Loss = -np.mean(np.sum(np.log(fast_sigmoid_pos_uv))+np.sum(np.log(1-fast_sigmoid_neg_uv),axis=1))\n",
    "    print(\"Loss:\",Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 注意这里loss的写法：因为是batch_size所以用矩阵算loss。\n",
    "    就要用到np.sum，因为loss是一个数值。\n",
    "    sum时考虑除了batch_size维以外的所有维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATING...\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'pos_u' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32mcell_name\u001b[0m in \u001b[0;36masync-def-wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'pos_u' referenced before assignment"
     ]
    }
   ],
   "source": [
    "\n",
    "    #对四个embedding 向量分别更新一次\n",
    "    print(\"UPDATING...\")\n",
    "    r = 0\n",
    "    r += np.sum(grad_pos_u*grad_pos_u)\n",
    "    d_pos_u = (-lr/(epsilon+np.sqrt(r)))*grad_pos_u\n",
    "    assert(d_pos_u.shape == pos_u.shape)\n",
    "    pos_u += d_pos_u\n",
    "    r = 0\n",
    "    r += np.sum(grad_pos_v*grad_pos_v)\n",
    "    d_pos_v = (-lr/(epsilon+np.sqrt(r)))*grad_pos_v\n",
    "    assert(d_pos_v.shape == pos_v.shape)\n",
    "    pos_v += d_pos_v\n",
    "    r = 0\n",
    "    r += np.sum(grad_neg_u*grad_neg_u)\n",
    "    d_neg_u = (-lr/(epsilon+np.sqrt(r)))*grad_neg_u\n",
    "    assert(d_neg_u.shape == grad_neg_u.shape)\n",
    "    neg_u += d_neg_u.reshape(BATCH_SIZE,NEG_RATIO,REP_SIZE)\n",
    "    r = 0\n",
    "    r += np.sum(grad_neg_v*grad_neg_v)\n",
    "    d_neg_v = (-lr/(epsilon+np.sqrt(r)))*grad_neg_v\n",
    "    assert(d_neg_v.shape == grad_neg_v.shape)\n",
    "    neg_v += d_neg_v.reshape(BATCH_SIZE,NEG_RATIO,REP_SIZE)\n",
    "    print(\"UPDATE FINISH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里有一个trick: neg的原始维度是三维，但是d_neg是二维的，因为把前2维合并为1维了：(128,5)->640.因为grad_neg是对每个负节点的向量求梯度。而负节点的数目是128*5。<br>\n",
    "为什么neg_u和neg_v是三维？这是源代码构建方法导致的。构建一个batch的样本时，每次挑出一个正的u编号，以及连边的一个正的v编号。同时按照分布挑选neg_ratio(K)数量的负u编号,负v编号。然后再用这些编号去所有节点的embedding矩阵(#NODE_NUM,128)中取向量。<br>\n",
    "所以此时正的u,v编号是一维列表(BATCH_SIZE,1)可以直接取,取出来的就是(BATCH_SIZE,128)。负的u,v编号是二维列表(BATCH_SIZE,K),从矩阵里面取就是三维(BATCH_SIZE,K,128),因为是对每一个节点都取128维同时保证原维度不变。<br>\n",
    "所以更新neg_u和neg_v时先把d_neg reshape为三维的，再加回去。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 核心算法2： batch的构建方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "源码把获取batch的函数做成了一个迭代器，即batch_iter()\n",
    "大致流程如下：<br>\n",
    "  total_sample_num = xxx <br>\n",
    "  start_idx = 0 <br>\n",
    "  end_idx = min(start_idx+batch_size,totak_sample_num) <br>\n",
    "  while end_idx < total_sample_num: <br>\n",
    "      for i in range(start_idx, end_idx):<br>\n",
    "          get batch<br>\n",
    "      start_idx = end_idx<br>\n",
    "      end_idx = min(start_idx+batch_size,totak_sample_num) <br>\n",
    "      yield batch<br>\n",
    "每一个执行batch_iter()后都用yield返回结果，同时迭代器内部返回while行等待下一次被调用。因为yield前已经更新了start和end，所以下一次取值时就从新的下标开始，实现了迭代。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 核心算法3： 节点正负样本采样方法 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正样本：两点存在边，则它们俩分别是正样本u  正样本v<br>\n",
    "负样本：对所有节点，按照概率分布判断是不是负样本，按照负采样数量每次采相应数量的节点。<br>\n",
    "负样本组成是原先的正样本u（在这里起别名为负样本u）和按照上面规则挑出的负样本v。由此可见，我们认为u和v之间没有边。但实际上可能有边，因为我们是按照概率挑选的，不是实际去看非v邻居的那些点，开销太大了。<br>\n",
    "关于负采样概率："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {0: 0,\n",
       "             1: 14,\n",
       "             2: 4,\n",
       "             3: 16,\n",
       "             4: 10,\n",
       "             5: 19,\n",
       "             6: 7,\n",
       "             7: 14,\n",
       "             8: 6,\n",
       "             9: 16,\n",
       "             10: 19,\n",
       "             11: 8,\n",
       "             12: 14,\n",
       "             13: 3,\n",
       "             14: 16,\n",
       "             15: 13,\n",
       "             16: 15,\n",
       "             17: 17,\n",
       "             18: 12,\n",
       "             19: 6})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#模拟生成节点和度\n",
    "nodeNum = 20\n",
    "nodes = np.arange(nodeNum)\n",
    "degree = defaultdict(int)\n",
    "for v in nodes:\n",
    "    degree[v] = np.random.randint(0,20)\n",
    "degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个节点被负采样的概率p = dv^0.75/sum(di^0.75,i)，sum就是代码里面的norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1.  1.  1.  2.  2.  2.  3.  3.  3.  3.  3.  3.  4.  4.  4.\n",
      "  4.  4.  5.  5.  5.  5.  5.  5.  5.  5.  6.  6.  6.  7.  7.  7.  7.  7.\n",
      "  7.  8.  8.  8.  9.  9.  9.  9.  9.  9.  9. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 11. 11. 11. 11. 12. 12. 12. 12. 12. 12. 13. 14. 14. 14. 14. 14. 14.\n",
      " 14. 15. 15. 15. 15. 15. 15. 16. 16. 16. 16. 16. 16. 17. 17. 17. 17. 17.\n",
      " 17. 17. 18. 18. 18. 18. 18. 19. 19. 19.]\n"
     ]
    }
   ],
   "source": [
    "#首先建立负采样表，采集了若干节点\n",
    "table_size = 100\n",
    "neg_table = np.zeros(table_size) #开辟一块空间\n",
    "norm = np.sum(np.power(np.array(list(degree.values())),0.75))\n",
    "p = 0\n",
    "i = 0\n",
    "for j in range(nodeNum):\n",
    "    p += np.power(degree[j],0.75)/norm  #没有归一化的化，不仅不是概率，而且会使p很大，整个表都填充第一个节点（absolutely NG!）\n",
    "    while i<table_size and float(i)<p*table_size:\n",
    "        neg_table[i] = j\n",
    "        i += 1\n",
    "print(neg_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何实现采样表的？<br>\n",
    "要注意那个while循环，之前一直忽略了。首先neg_table的长度要很大很大，相当于开辟了一块空间。然后每个节点的被负采样概率p*table_size就是在table里占的格子数，就是while里面的东西。当下标＜格子数时，每个格子就一直放这个被采样的节点，直到它的格子数用完再去采下一个。<br>\n",
    "注意为什么p是累加的？因为格子数和下标i一直在累加。neg_table的样子如下：假设前三个被采样到的节点是1，3，5：<br>\n",
    "1 1 1 3 3 3 3 5 5 5 5 5...(占多少格子是瞎编的，具体可以看上面代码的运行结果)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 10, 2, 17, 5]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#然后在负采样表里拿节点\n",
    "neg_ratio = 5 #一次拿这些个\n",
    "neg_v = list() #neg_u就是[pos_u,pos_u,pos_u,pos_u,pos_u]\n",
    "for i in range(neg_ratio):\n",
    "    neg_v.append(int(neg_table[np.random.randint(0,table_size-1)]))\n",
    "neg_v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "某次结果：neg_v=[18, 1, 2, 9, 9]，有重复节点，所以还要一系列去重操作.最简单的就是“有重复就一直取”。源代码还要求节点不能是neg_u里的。等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 3, 10, 16, 18]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#带去重的负采样表拿节点\n",
    "neg_ratio = 5 #一次拿这些个\n",
    "neg_v = list() #neg_u就是[pos_u,pos_u,pos_u,pos_u,pos_u]\n",
    "for i in range(neg_ratio):\n",
    "    v = int(neg_table[np.random.randint(0,table_size-1)])\n",
    "    while v in neg_v: #有重复就一直取\n",
    "        v = int(neg_table[np.random.randint(0,table_size-1)])\n",
    "    neg_v.append(v)\n",
    "neg_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 核心思想：降维是什么"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "降维就是压缩，压缩就要还原。<br>\n",
    "所以使用sigmoid函数对边存在概率建模，建立目标函数，同时考虑正负样本。正样本是指原来存在边的，那么它降维后再计算sigmoid就要越大越好，代表降维后也有边。\n",
    "负样本指原来不存在边的，那么最大化1-sigmoid就好，因为需要sigmoid尽量小。所以有了这样的目标函数，取相反数变成loss再最小化。<br>\n",
    "使用负采样是因为图的负样本太多了，不平衡。假如在一个分类问题中，负样本过多，那么分类器只会更多地学习负样本特征，抓不到正样本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 工程技巧：如何组织模型代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PALE源代码（embedding部分里），首先定义一个类，封装的是模型，包含各种成员及方法。<br>\n",
    "最重要的方法就是,batch_iter(),train_one_epoch()，其他方法都是服务于论文思想的。\n",
    "再定义一个执行类，在__init()__里实例化model，并执行train_one_epoch()开始训练。然后模型其他方法传出来的各种信息可以在这里记录下。比如\n",
    "模型有get_vectors()拿到每一次embedding，调用这个方法可以把每一个epoch的结果写成文件。<br>\n",
    "当然，模型中可能还有预测什么的方法。具体看情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pale.py源码里面的一些技巧：<br>\n",
    "lookup字典，记录节点标号---->0开始的连续值,在读取embedding文件/图文件时使用，如对每一行循环：<br>\n",
    "LOOP:<br>\n",
    "idx = 0 <br>\n",
    "lookup[idx] = line[0] <br>\n",
    "emb[line[0]] = map(float,line[1:]) <br>\n",
    "idx += 1 <br>\n",
    "每一行第一个数字是节点标号，后面的128(e.g.)个数字是embedding的每个维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体的tf搭模型方法等真正开始搭的时候再来看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
